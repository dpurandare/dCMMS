apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: predictive-maintenance-model
  namespace: dcmms-ml
  annotations:
    autoscaling.knative.dev/minScale: "1"
    autoscaling.knative.dev/maxScale: "5"
    autoscaling.knative.dev/target: "10"  # Target 10 concurrent requests per pod
spec:
  predictor:
    # Model format: MLflow
    model:
      modelFormat:
        name: mlflow
      storageUri: s3://dcmms-models/mlflow-artifacts  # Or: gs://, file://, etc.
      runtime: kserve-mlflowserver  # Or custom Python server

      # Resource requests and limits
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"
        limits:
          cpu: "2"
          memory: "4Gi"

      # Environment variables
      env:
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow-server:5000"
        - name: MODEL_NAME
          value: "predictive_maintenance_best"
        - name: MODEL_VERSION
          value: "Production"  # Or specific version number

      # Health checks
      livenessProbe:
        httpGet:
          path: /health/live
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3

      readinessProbe:
        httpGet:
          path: /health/ready
          port: 8080
        initialDelaySeconds: 10
        periodSeconds: 5
        timeoutSeconds: 3
        failureThreshold: 3

  # Canary deployment (optional)
  # Uncomment to enable canary rollout
  # canaryTrafficPercent: 10  # Route 10% traffic to canary version

---
# HorizontalPodAutoscaler for auto-scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: predictive-maintenance-model-hpa
  namespace: dcmms-ml
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: predictive-maintenance-model-predictor
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Pods
      pods:
        metric:
          name: requests_per_second
        target:
          type: AverageValue
          averageValue: "100"  # Scale when RPS > 100 per pod

---
# ServiceMonitor for Prometheus metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: predictive-maintenance-model-monitor
  namespace: dcmms-ml
  labels:
    app: predictive-maintenance-model
spec:
  selector:
    matchLabels:
      serving.kserve.io/inferenceservice: predictive-maintenance-model
  endpoints:
    - port: metrics
      path: /metrics
      interval: 15s
